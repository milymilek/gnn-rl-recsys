{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a88688-5214-4ead-a060-4df9633efd89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milosz\\Desktop\\python\\thesis-recsys\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"../../\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b41f0f27-4278-4c12-845c-2655b9edcad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b9ab975490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "import operator\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.functional import pad\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec6ccb1-bebd-4890-adb6-64417a22ed65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2af40eaa-9d12-46b3-8ffd-3115d7134536",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/steam/data.pkl\", \"rb\") as f:\n",
    "    data = pd.read_pickle(f)\n",
    "    #data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0924f7d8-7845-40a4-b0e5-0b347ff45457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_col = 'user_id'\n",
    "item_col = 'app_id'\n",
    "train_set = data['train_set'][[user_col, item_col]].values.T\n",
    "supervision_set = data['supervision_set'][[user_col, item_col]].values.T\n",
    "valid_set = data['valid_set'][[user_col, item_col]].values.T\n",
    "user_attr = data['user_attr']\n",
    "item_attr = data['item_attr']\n",
    "\n",
    "train_set = np.concatenate([train_set, supervision_set], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec240adc-0d43-4a65-b58c-16f2e5174f33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scaler_user = StandardScaler()\n",
    "# user_attr_preprocess = scaler_user.fit_transform(user_attr)\n",
    "\n",
    "# scaler_item = StandardScaler()\n",
    "# item_attr_preprocess = np.copy(item_attr)\n",
    "# item_attr_preprocess[:, 435:] = scaler_item.fit_transform(item_attr[:, 435:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "263f75ed-1839-436f-8189-ce7330522ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def temp_item_attr_rebuild(item_attr):\n",
    "    new_item_attr = []\n",
    "    for row in item_attr:\n",
    "        tags = row[15]\n",
    "        OS = list(row[0:3].nonzero()[0])\n",
    "        avg_rat = row[4:10].nonzero()[0][0]\n",
    "        price_original = row[12]\n",
    "        new_item_attr.append([tags, OS, avg_rat, price_original])\n",
    "    return np.array(new_item_attr, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1468d58-223c-401b-8744-b57a11308b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_attr2 = temp_item_attr_rebuild(item_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf87c72a-70b9-4dfd-978f-176443a485ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([382, 345, 380, 228, 169, 12, 21, 322, 167, 157, 295, 74, 120, 356, 110, 222, 321, 72, 158, 370]),\n",
       "       list([0, 1, 2]), 3, 29.99], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_attr2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686299d7-371c-46a0-bf2f-2282b7cfb8b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheme = {\n",
    "    \"user_id\": {\n",
    "        \"emb_dim\": 8,\n",
    "        \"num_emb\": user_attr.shape[0],\n",
    "        \"max_len\": 1,\n",
    "        \"type\": 'sparse'\n",
    "    },\n",
    "    \"item_id\": {\n",
    "        \"emb_dim\": 8,\n",
    "        \"num_emb\": item_attr.shape[0],\n",
    "        \"max_len\": 1,\n",
    "        \"type\": 'sparse'\n",
    "    },\n",
    "    \"tags\": {\n",
    "        \"emb_dim\": 8,\n",
    "        \"num_emb\": 425,\n",
    "        \"max_len\": 20,\n",
    "        \"type\": 'sparse'\n",
    "    },\n",
    "    \"OS\": {\n",
    "        \"emb_dim\": 8,\n",
    "        \"num_emb\": 3,\n",
    "        \"max_len\": 3,\n",
    "        \"type\": 'sparse'\n",
    "    },\n",
    "    \"AvgCatRating\": {\n",
    "        \"emb_dim\": 8,\n",
    "        \"num_emb\": 6,\n",
    "        \"max_len\": 1,\n",
    "        \"type\": 'sparse'\n",
    "    },\n",
    "    \"PriceOriginal\": {\n",
    "        \"type\": 'dense'\n",
    "    }\n",
    "}\n",
    "\n",
    "class SparseFeat(namedtuple('SparseFeat', ['name', 'emb_dim', 'num_emb', \"max_len\", \"index\", 'pad_index'])):\n",
    "    def __new__(cls, name, emb_dim, num_emb, max_len, index, pad_index):\n",
    "        return super(SparseFeat, cls).__new__(cls, name, emb_dim, num_emb, max_len, index, pad_index)\n",
    "    \n",
    "class DenseFeat(namedtuple('DenseFeat', ['name', \"index\", 'pad_index'])):\n",
    "    def __new__(cls, name, index, pad_index):\n",
    "        return super(DenseFeat, cls).__new__(cls, name, index, pad_index)    \n",
    "    \n",
    "# class VarLenSparseFeat(namedtuple('VarLenSparseFeat', ['name', 'emb_dim', 'num_emb', \"max_len\", 'index'])):\n",
    "#     def __new__(cls, name, emb_dim, num_emb, max_len, index):\n",
    "#         return super(VarLenSparseFeat, cls).__new__(cls, name, emb_dim, num_emb, max_len, index)\n",
    "\n",
    "# TODO: start+=feat['max_len'] powoduje blad, do sprawdzenia\n",
    "class FeatureStore:\n",
    "    def __init__(self, scheme):\n",
    "        self.features = {'sparse': [], 'dense': []}\n",
    "        self.n_features = len(scheme.keys())\n",
    "        \n",
    "        start = 0\n",
    "        for i, (feat_name, feat) in enumerate(scheme.items()):\n",
    "            if feat['type'] == 'sparse':\n",
    "                self.features['sparse'].append(\n",
    "                    SparseFeat(feat_name, feat['emb_dim'], feat['num_emb'], feat['max_len'], i, (start, start+feat['max_len']))\n",
    "                )\n",
    "                start = start + feat['max_len']\n",
    "            elif feat['type'] == 'dense':\n",
    "                self.features['dense'].append(\n",
    "                    DenseFeat(feat_name, i, (start, start+1))\n",
    "                )\n",
    "                start += 1\n",
    "\n",
    "        self.sparse_index = self.get_feature_index('sparse')\n",
    "        self.dense_index = self.get_feature_index('dense')\n",
    "\n",
    "    def input_len(self):\n",
    "        sparse_len = sum([f.emb_dim for f in self.features['sparse']])\n",
    "        dense_len = len(self.features['dense'])\n",
    "        return sparse_len + dense_len\n",
    "    \n",
    "    def emb_dim(self):\n",
    "        return 8\n",
    "    \n",
    "    def num_emb(self):\n",
    "        return sum([f.num_emb for f in self.features['sparse']])\n",
    "\n",
    "    def get_feature_index(self, feat_type):\n",
    "        return torch.hstack([torch.arange(i[0], i[1]) for i in [f.pad_index for f in self.features[feat_type]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0870a85b-b7ac-4eb9-b49b-5b6caedf8fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_store = FeatureStore(scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31739385-daee-4693-ad46-180eb9c775d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepFMDataset(Dataset):\n",
    "    def __init__(self, edge_index, user_attr, item_attr):\n",
    "        self.edge_index = edge_index\n",
    "        self.user_attr = user_attr\n",
    "        self.item_attr = item_attr\n",
    "        \n",
    "        self.users = edge_index[0]\n",
    "        self.items = edge_index[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.edge_index.shape[1]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        u_id = self.users[idx]\n",
    "        i_id = self.items[idx]\n",
    "        \n",
    "        #u_attr = self.user_attr[u_id]\n",
    "        u_attr = []\n",
    "        i_attr = self.item_attr[i_id]\n",
    "\n",
    "        x = np.concatenate([[u_id], [i_id], u_attr, i_attr])\n",
    "        \n",
    "        sparse_features = self._get_sparse_features(x)\n",
    "        dense_features = self._get_dense_features(x)\n",
    "        \n",
    "        return sparse_features, dense_features\n",
    "    \n",
    "    def _get_sparse_features(self, x):\n",
    "        indices = [f.index for f in feature_store.features['sparse']]\n",
    "        return x[indices]\n",
    "    \n",
    "    def _get_dense_features(self, x):\n",
    "        indices = [f.index for f in feature_store.features['dense']]\n",
    "        return x[indices]\n",
    "\n",
    "def pad_sequence_max_len(sequence, max_len):\n",
    "    sequence[0] = pad(sequence[0], (0, max_len-sequence[0].shape[0]), value=-1)\n",
    "    return pad_sequence(sequence, batch_first=True, padding_value=-1) + 1\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xs = [[] for f in range(feature_store.n_features)]\n",
    "    for (_sparse, _dense) in batch:\n",
    "        for i, f in enumerate(feature_store.features['sparse']):\n",
    "            xs[f.index].append(torch.tensor(_sparse[i], dtype=torch.float32))\n",
    "\n",
    "        for i, f in enumerate(feature_store.features['dense']):\n",
    "            xs[f.index].append(torch.tensor(_dense[i], dtype=torch.float32))\n",
    "\n",
    "    for i, f in enumerate(feature_store.features['sparse']):\n",
    "        if f.max_len > 1:\n",
    "            xs[f.index] = pad_sequence_max_len(xs[f.index], f.max_len)\n",
    "        else:\n",
    "            xs[f.index] = torch.tensor(xs[f.index]) + 1\n",
    "        \n",
    "    for i, f in enumerate(feature_store.features['dense']):\n",
    "        xs[f.index] = torch.tensor(xs[f.index])\n",
    "    \n",
    "    xs = torch.column_stack(xs)\n",
    "\n",
    "    return xs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca648d8-4f41-4dfb-a0ce-1f8c062430b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = DeepFMDataset(train_set, user_attr, item_attr2)\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=1024, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5119b48-cc47-457a-95de-9e535e8c2836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e89788e-35b0-4b42-b95a-e95d7156c7c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_size):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.u = nn.Embedding(n_users, emb_size)\n",
    "        self.i = nn.Embedding(n_items, emb_size)\n",
    "        \n",
    "    def forward(self, ux, ix):\n",
    "        return torch.sum(self.u(ux) * self.i(ix), dim=1)\n",
    "        #return torch.sum((self.u(ux) * self.i(ix)).squeeze(1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9aed4391-d8bf-494b-9275-6766aeaeec44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseFeat(name='user_id', emb_dim=8, num_emb=3066721, max_len=1, index=0, pad_index=(0, 1)),\n",
       " SparseFeat(name='item_id', emb_dim=8, num_emb=1231, max_len=1, index=1, pad_index=(1, 2)),\n",
       " SparseFeat(name='tags', emb_dim=8, num_emb=425, max_len=20, index=2, pad_index=(2, 22)),\n",
       " SparseFeat(name='OS', emb_dim=8, num_emb=3, max_len=3, index=3, pad_index=(22, 25)),\n",
       " SparseFeat(name='AvgCatRating', emb_dim=8, num_emb=6, max_len=1, index=4, pad_index=(25, 26))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_store.features['sparse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3141a2b0-dc80-45c9-9f64-b0dc9b2882f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class EmbeddingBagCollection:\n",
    "#     \"\"\"\n",
    "#     EmbeddingBagCollection represents a collection of pooled embeddings (`EmbeddingBags`).\n",
    "\n",
    "#     It processes sparse data in the form of `KeyedJaggedTensor` with values of the form\n",
    "#     [F X B X L] where:\n",
    "\n",
    "#     * F: features (keys)\n",
    "#     * B: batch size\n",
    "#     * L: length of sparse features (jagged)\n",
    "\n",
    "#     and outputs a `KeyedTensor` with values of the form [B * (F * D)] where:\n",
    "\n",
    "#     * F: features (keys)\n",
    "#     * D: each feature's (key's) embedding dimension\n",
    "#     * B: batch size\n",
    "\n",
    "#     Args:\n",
    "#         tables (List[EmbeddingBagConfig]): list of embedding tables.\n",
    "#         is_weighted (bool): whether input `KeyedJaggedTensor` is weighted.\n",
    "#         device (Optional[torch.device]): default compute device.\n",
    "\n",
    "#     Example::\n",
    "\n",
    "#         table_0 = EmbeddingBagConfig(\n",
    "#             name=\"t1\", embedding_dim=3, num_embeddings=10, feature_names=[\"f1\"]\n",
    "#         )\n",
    "#         table_1 = EmbeddingBagConfig(\n",
    "#             name=\"t2\", embedding_dim=4, num_embeddings=10, feature_names=[\"f2\"]\n",
    "#         )\n",
    "\n",
    "#         ebc = EmbeddingBagCollection(tables=[table_0, table_1])\n",
    "\n",
    "#         #        0       1        2  <-- batch\n",
    "#         # \"f1\"   [0,1] None    [2]\n",
    "#         # \"f2\"   [3]    [4]    [5,6,7]\n",
    "#         #  ^\n",
    "#         # feature\n",
    "\n",
    "#         features = KeyedJaggedTensor(\n",
    "#             keys=[\"f1\", \"f2\"],\n",
    "#             values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n",
    "#             offsets=torch.tensor([0, 2, 2, 3, 4, 5, 8]),\n",
    "#         )\n",
    "\n",
    "#         pooled_embeddings = ebc(features)\n",
    "#         print(pooled_embeddings.values())\n",
    "#         tensor([[-0.8899, -0.1342, -1.9060, -0.0905, -0.2814, -0.9369, -0.7783],\n",
    "#             [ 0.0000,  0.0000,  0.0000,  0.1598,  0.0695,  1.3265, -0.1011],\n",
    "#             [-0.4256, -1.1846, -2.1648, -1.0893,  0.3590, -1.9784, -0.7681]],\n",
    "#             grad_fn=<CatBackward0>)\n",
    "#         print(pooled_embeddings.keys())\n",
    "#         ['f1', 'f2']\n",
    "#         print(pooled_embeddings.offset_per_key())\n",
    "#         tensor([0, 3, 7])\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, feature_store: FeatureStore) -> None:\n",
    "#         super().__init__()\n",
    "#         self.embedding_bags: nn.ModuleDict = nn.ModuleDict()\n",
    "#         self._feature_store = feature_store\n",
    "\n",
    "#         for feature_config in feature_store.features['sparse']:\n",
    "#             self.embedding_bags[feature_config.name] = nn.Embedding(\n",
    "#                 num_embeddings=feature_config.num_emb,\n",
    "#                 embedding_dim=embedding_config.emb_dim,\n",
    "#                 padding_idx=-1,\n",
    "#                 dtype=torch.float32,\n",
    "#                 device=device,\n",
    "#             )\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             features (KeyedJaggedTensor): KJT of form [F X B X L].\n",
    "\n",
    "#         Returns:\n",
    "#             KeyedTensor\n",
    "#         \"\"\"\n",
    "\n",
    "#         pooled_embeddings: List[torch.Tensor] = []\n",
    "\n",
    "#         feature_dict = features.to_dict()\n",
    "#         for i, embedding_bag in enumerate(self.embedding_bags.values()):\n",
    "#             for feature_name in self._feature_names[i]:\n",
    "#                 f = feature_dict[feature_name]\n",
    "#                 res = embedding_bag(\n",
    "#                     input=f.values(),\n",
    "#                     offsets=f.offsets(),\n",
    "#                     per_sample_weights=f.weights() if self._is_weighted else None,\n",
    "#                 ).float()\n",
    "#                 pooled_embeddings.append(res)\n",
    "#         data = torch.cat(pooled_embeddings, dim=1)\n",
    "#         return KeyedTensor(\n",
    "#             keys=self._embedding_names,\n",
    "#             values=data,\n",
    "#             length_per_key=self._lengths_per_embedding,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "501f9902-3592-4ce0-b29e-12d4c6102ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CrossProductNet(nn.Module):\n",
    "    def forward(self, emb_x):\n",
    "        cross = emb_x @ emb_x.permute(1, 0)\n",
    "        triangle_lower_cross = torch.tril(cross, diagonal=-1)\n",
    "        return torch.sum(triangle_lower_cross)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.1, batch_norm=True):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        _layers = []\n",
    "        dims = [input_dim] + hidden_dim + [1]\n",
    "        for in_dim, out_dim in zip(dims, dims[1:]):\n",
    "            _layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if batch_norm:\n",
    "                _layers.append(nn.BatchNorm1d(out_dim))\n",
    "            _layers.append(nn.ReLU())\n",
    "            _layers.append(nn.Dropout(p=dropout))\n",
    "\n",
    "        self.layers = nn.ModuleList(_layers)\n",
    "        \n",
    "    def forward(self, emb_x):\n",
    "        return self.layers(emb_x)\n",
    "    \n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, feature_store):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        _embeddings = {feat.name: nn.Embedding(feat.num_emb, feat.emb_dim, padding_idx=0, device=device) for feat in feature_store.features['sparse']}\n",
    "        self._feature_store = feature_store\n",
    "        self.embeddings = nn.ModuleDict(_embeddings)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_emb = []\n",
    "        for name, embed_matrix in self.embeddings.items():\n",
    "            feat = [i for i in self._feature_store.features['sparse'] if i.name == name][0]\n",
    "            emb = embed_matrix(x[feat.pad_index[0]:feat.pad_index[1]].to(torch.long))\n",
    "            print(f\"FEAT: {feat} \\nX: {x[feat.pad_index[0]:feat.pad_index[1]].to(torch.long)}\\n X_EMB: {emb}\\n\")\n",
    "            emb_agg = torch.sum(emb, axis=0)\n",
    "            print(emb_agg)\n",
    "            x_emb.append(emb_agg)\n",
    "        x_emb = torch.cat(x_emb, axis=1)\n",
    "        return emb   \n",
    "\n",
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, feature_store, hidden_dim):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.feature_store = feature_store\n",
    "\n",
    "        self.V = EmbeddingNet(feature_store)\n",
    "        self.fm = CrossProductNet()\n",
    "        self.dnn = MLP(input_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_dense = x[feature_store.dense_index]\n",
    "        \n",
    "        x_sparse = self.V(x)\n",
    "        x_sparse_dense = torch.cat([x_sparse, x_dense])\n",
    "        x = self.fm(x_sparse_emb) + self.dnn(x_sparse_dense)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa1bd599-6075-416d-95f1-55801b833c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "en = EmbeddingNet(feature_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3989a088-0c76-4dc9-ae1a-728c010ca909",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (user_id): Embedding(3066721, 8, padding_idx=0)\n",
       "  (item_id): Embedding(1231, 8, padding_idx=0)\n",
       "  (tags): Embedding(425, 8, padding_idx=0)\n",
       "  (OS): Embedding(3, 8, padding_idx=0)\n",
       "  (AvgCatRating): Embedding(6, 8, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80407edd-8a11-4651-a1f4-98e8dc39b39b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sparse': [SparseFeat(name='user_id', emb_dim=8, num_emb=3066721, max_len=1, index=0, pad_index=(0, 1)),\n",
       "  SparseFeat(name='item_id', emb_dim=8, num_emb=1231, max_len=1, index=1, pad_index=(1, 2)),\n",
       "  SparseFeat(name='tags', emb_dim=8, num_emb=425, max_len=20, index=2, pad_index=(2, 22)),\n",
       "  SparseFeat(name='OS', emb_dim=8, num_emb=3, max_len=3, index=3, pad_index=(22, 25)),\n",
       "  SparseFeat(name='AvgCatRating', emb_dim=8, num_emb=6, max_len=1, index=4, pad_index=(25, 26))],\n",
       " 'dense': [DenseFeat(name='PriceOriginal', index=5, pad_index=(26, 27))]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_store.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ead9413-9b13-49a5-ab5d-4979c84f0715",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2.00,   2.00,  17.00, 162.00,  65.00,  23.00, 159.00, 348.00, 340.00,\n",
       "        323.00, 362.00,  47.00,  88.00, 109.00, 105.00, 134.00,  37.00, 345.00,\n",
       "        285.00,   0.00,   0.00,   0.00,   1.00,   0.00,   0.00,   6.00,  19.99],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d39de69a-d219-49f2-bcbc-289319109eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEAT: SparseFeat(name='user_id', emb_dim=8, num_emb=3066721, max_len=1, index=0, pad_index=(0, 1)) \n",
      "X: tensor([2], device='cuda:0')\n",
      " X_EMB: tensor([[-0.13,  1.22,  1.44,  1.06, -0.49, -1.42, -0.72, -1.30]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "tensor([-0.13,  1.22,  1.44,  1.06, -0.49, -1.42, -0.72, -1.30],\n",
      "       device='cuda:0', grad_fn=<SumBackward1>)\n",
      "FEAT: SparseFeat(name='item_id', emb_dim=8, num_emb=1231, max_len=1, index=1, pad_index=(1, 2)) \n",
      "X: tensor([2], device='cuda:0')\n",
      " X_EMB: tensor([[-0.50,  0.59,  1.34,  1.16, -0.18,  0.08,  0.91, -1.04]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "tensor([-0.50,  0.59,  1.34,  1.16, -0.18,  0.08,  0.91, -1.04],\n",
      "       device='cuda:0', grad_fn=<SumBackward1>)\n",
      "FEAT: SparseFeat(name='tags', emb_dim=8, num_emb=425, max_len=20, index=2, pad_index=(2, 22)) \n",
      "X: tensor([ 17, 162,  65,  23, 159, 348, 340, 323, 362,  47,  88, 109, 105, 134,\n",
      "         37, 345, 285,   0,   0,   0], device='cuda:0')\n",
      " X_EMB: tensor([[-0.99,  0.67, -1.79, -0.65,  0.26, -0.81,  0.32,  0.27],\n",
      "        [-0.56, -0.62, -0.21,  0.27,  1.63, -1.01,  1.67, -1.58],\n",
      "        [ 0.82,  0.21, -0.20,  0.57,  0.34, -1.00,  1.21,  0.50],\n",
      "        [-0.23,  0.00,  1.14, -0.69, -1.36,  0.80, -0.35,  1.41],\n",
      "        [ 0.12, -1.15, -0.24,  0.24, -0.18,  0.43, -1.07,  0.09],\n",
      "        [-0.08,  0.15, -2.09,  0.73, -1.57, -0.23, -0.24,  0.85],\n",
      "        [ 1.26,  0.74, -0.70, -0.29,  1.90,  1.35,  0.70, -0.23],\n",
      "        [ 0.06, -2.23, -0.68,  0.87,  0.06,  0.63, -0.55,  0.44],\n",
      "        [-0.21,  0.69,  1.73,  0.62,  1.83,  0.65, -0.22,  0.45],\n",
      "        [-0.59,  0.01,  0.72, -0.34,  0.73,  0.56,  0.19,  0.52],\n",
      "        [-0.93,  1.42, -0.44, -0.06,  0.89, -0.50,  0.43,  0.70],\n",
      "        [ 1.34, -0.61,  1.19, -0.85, -1.05, -0.65,  0.32,  0.98],\n",
      "        [ 0.77, -0.17,  0.24,  0.17,  1.34, -0.17,  0.66, -0.24],\n",
      "        [ 0.74, -0.15, -0.23,  0.68,  1.74,  0.41,  0.46,  1.05],\n",
      "        [ 0.84, -0.71, -1.61, -0.24, -0.69, -1.51,  0.83, -1.13],\n",
      "        [ 0.09, -0.83,  0.24, -1.55, -0.73,  0.63,  0.32, -0.69],\n",
      "        [-0.76,  0.54,  0.23,  0.22,  0.69, -1.96, -1.18, -0.36],\n",
      "        [ 0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00],\n",
      "        [ 0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00],\n",
      "        [ 0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "tensor([ 1.69, -2.01, -2.69, -0.30,  5.81, -2.40,  3.50,  3.03],\n",
      "       device='cuda:0', grad_fn=<SumBackward1>)\n",
      "FEAT: SparseFeat(name='OS', emb_dim=8, num_emb=3, max_len=3, index=3, pad_index=(22, 25)) \n",
      "X: tensor([1, 0, 0], device='cuda:0')\n",
      " X_EMB: tensor([[-0.96,  0.25, -0.25,  1.56,  1.44, -1.69,  1.33, -0.25],\n",
      "        [ 0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00],\n",
      "        [ 0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "tensor([-0.96,  0.25, -0.25,  1.56,  1.44, -1.69,  1.33, -0.25],\n",
      "       device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43men\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\recsys\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[18], line 37\u001b[0m, in \u001b[0;36mEmbeddingNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m feat \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_store\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m name][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     36\u001b[0m emb \u001b[38;5;241m=\u001b[39m embed_matrix(x[feat\u001b[38;5;241m.\u001b[39mpad_index[\u001b[38;5;241m0\u001b[39m]:feat\u001b[38;5;241m.\u001b[39mpad_index[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong))\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFEAT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mX: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx[feat\u001b[38;5;241m.\u001b[39mpad_index[\u001b[38;5;241m0\u001b[39m]:feat\u001b[38;5;241m.\u001b[39mpad_index[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m X_EMB: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00memb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m emb_agg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(emb, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(emb_agg)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\recsys\\lib\\site-packages\\torch\\_tensor.py:859\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m--> 859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\recsys\\lib\\site-packages\\torch\\_tensor.py:427\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    424\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[0;32m    425\u001b[0m     )\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\recsys\\lib\\site-packages\\torch\\_tensor_str.py:637\u001b[0m, in \u001b[0;36m_str\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    636\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\recsys\\lib\\site-packages\\torch\\_tensor_str.py:568\u001b[0m, in \u001b[0;36m_str_intern\u001b[1;34m(inp, tensor_contents)\u001b[0m\n\u001b[0;32m    566\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[0;32m    567\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 568\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[0;32m    571\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\recsys\\lib\\site-packages\\torch\\_tensor_str.py:328\u001b[0m, in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[0;32m    326\u001b[0m     )\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\recsys\\lib\\site-packages\\torch\\_tensor_str.py:111\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloating_dtype:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m tensor_view:\n\u001b[1;32m--> 111\u001b[0m         value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\recsys\\lib\\site-packages\\torch\\_tensor.py:858\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m--> 858\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "en(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b60322-6a67-44f3-a842-4108362cdc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepFM(feature_store, hidden_dim=[128,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd6081-ce07-4dae-994b-c089d2c5eaa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.tensor([[1,2,3], [4,5,6], [7,8,9]])[[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b9834-c97c-4a32-85f7-e03fd51deddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57202cec-db0d-47a3-a23b-4cc8851573b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65451b50-2b21-4b72-8954-95c53faf8f27",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset[9432]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0114530-1d6a-4353-8167-2683b2a31d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_input_types(train_dataset[9432], features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a6d8a-4aec-4366-a39b-7f8fad51e9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704dadb-a8d7-4257-abaa-4eeb8479c77e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "e = EmbeddingNet([f.num_emb for f in features], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d2037-ff6b-4cea-bfc8-b3524fd6ae50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.array((0, *np.cumsum([f.num_emb for f in features])[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937ac4f-e444-44c8-b70b-8c52cfc65fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[f.num_emb for f in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8c38a-3050-438d-a96d-f4666c214028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "e.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2499c-ce55-4ec8-9144-37e9dad129ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.arange(20).new_tensor([0, 5, 10]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3948e919-3d98-4e28-9f78-8ed8cad9b90d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_input = {\n",
    "    \"user_id\": 5,\n",
    "    \"item_id\": 20,\n",
    "    \"tags\": [0, 5, 8, 100],\n",
    "    \"OS\": [0, 1],\n",
    "    \"AvgCatRating\": 3 \n",
    "}\n",
    "x_input_tensor = torch.tensor([5, \n",
    "                               20, \n",
    "                               0, 5, 8, 100,\n",
    "                               0, 1,\n",
    "                               3\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b952a6b-4128-4cdf-a76c-b59b27a5421f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "e(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2608366-6dfa-45bb-bf72-e1c4a4e55138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def approx_negative_sampling(u_id, i_id, n_samples, bs):\n",
    "    ind = torch.arange(bs).repeat(n_samples)\n",
    "    perm =  torch.randperm(ind.shape[0])\n",
    "    random_indices = ind[perm]\n",
    "    i_id_neg = i_id[random_indices]\n",
    "    \n",
    "    u_id = u_id.repeat(n_samples+1)\n",
    "    i_id = torch.cat([i_id, i_id_neg])\n",
    "    y_true = torch.cat([torch.ones(bs).to(device),\n",
    "                       torch.zeros(bs*n_samples).to(device)])\n",
    "    \n",
    "    return u_id, i_id, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d8d8c-9497-4114-be44-dd32bf945e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(n_epochs, print_loss=500):\n",
    "    model.train()\n",
    "    batch_size = train_loader.batch_size\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.\n",
    "        preds, ground_truths = [], []\n",
    "        batch_print_loss = 0.\n",
    "        for i_batch, batch in enumerate(tqdm(train_loader)):\n",
    "            batch = [i.to(device) for i in batch]\n",
    "            u_id, i_id, u_attr, i_attr = batch\n",
    " \n",
    "            u_id, i_id, y_true = approx_negative_sampling(u_id, i_id, n_samples=2, bs=batch_size)\n",
    "            \n",
    "            y_pred = model(u_id, i_id)\n",
    "            loss = criterion(y_pred, y_true)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            preds.append(y_pred)\n",
    "            ground_truths.append(y_true)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if not ((i_batch+1) % print_loss):\n",
    "                pred = torch.cat(preds, dim=0).detach().sigmoid().cpu().numpy()\n",
    "                ground_truth = torch.cat(ground_truths, dim=0).detach().cpu().numpy()\n",
    "                last_loss = running_loss / print_loss\n",
    "                \n",
    "                train_roc_auc = roc_auc_score(ground_truth, pred)\n",
    "                test_loss, test_roc_auc = test()\n",
    "                \n",
    "                preds, ground_truths = [], []\n",
    "                running_loss = 0.\n",
    "                \n",
    "                print(f\"batch <{i_batch}>\\ntrain_loss: {last_loss} - train_roc_auc: {train_roc_auc}\\ntest_loss: {test_loss} - test_roc_auc: {test_roc_auc}\\n\")\n",
    "        print(f\"Epoch: {epoch}, Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79f0cd-9f28-498a-b764-cf0fb3117f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    batch_size = val_loader.batch_size\n",
    "    \n",
    "    running_loss = 0.\n",
    "    preds, ground_truths = [], []\n",
    "\n",
    "    for i_batch, batch in enumerate(tqdm(val_loader)):\n",
    "        batch = [i.to(device) for i in batch]\n",
    "        u_id, i_id, u_attr, i_attr = batch\n",
    "        \n",
    "        u_id, i_id, y_true = approx_negative_sampling(u_id, i_id, n_samples=2, bs=batch_size)\n",
    "\n",
    "        y_pred = model(u_id, i_id)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        \n",
    "        preds.append(y_pred)\n",
    "        ground_truths.append(y_true)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    \n",
    "    test_loss = running_loss / len(val_loader)\n",
    "    test_score = roc_auc_score(ground_truth, pred)\n",
    "\n",
    "    return test_loss, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29807538-6259-4fed-819b-acab8341ba21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = MFDataset(train_set, data['user_attr'], data['item_attr'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, drop_last=True)\n",
    "\n",
    "val_dataset = MFDataset(valid_set, data['user_attr'], data['item_attr'])\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e7247-d351-4d12-ad27-2b072f322bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_users, n_items = user_attr.shape[0], item_attr.shape[0]\n",
    "\n",
    "model = MF(n_users, n_items, emb_size=32)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42106db4-3626-44c7-b088-4fa4fd1144c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.RMSprop(params=model.parameters(), lr=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3c283-8dc0-4ea2-bc2f-6cc6ee1e9116",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(n_epochs=10, print_loss=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f3a58e-b222-4c5c-8aa4-6f3cd8be8b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabdfbbb-1002-4098-81a9-b00b4da58d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfecea9-ed11-42d9-87e3-c338303fa39d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch_size = train_loader.batch_size\n",
    "    batch = next(iter(train_loader))\n",
    "    batch = [i.to(device) for i in batch]\n",
    "    u_id, i_id, u_attr, i_attr = batch\n",
    "    \n",
    "    neg_item_idx = approx_negative_sampling(i_id, bs=batch_size)\n",
    "\n",
    "    u_id = torch.cat([u_id, u_id])\n",
    "    i_id = torch.cat([i_id, neg_item_idx])\n",
    "    y_true = torch.cat([torch.ones(batch_size).to(device),\n",
    "                        torch.zeros(batch_size).to(device)])\n",
    "\n",
    "    y_pred = model(u_id, i_id)\n",
    "    \n",
    "    print(y_pred.sigmoid(), y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca6fe9-55a6-4b7b-9ef6-48c1ef92aa5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7372b8f-3328-4d4f-a27d-bee6c00f76cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd7dd4-b196-4540-8eef-843f90ef523c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "def load_model(path):\n",
    "    model = MF(n_users, n_items, emb_size=32)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3819672-0ed1-4ab9-bd16-076184652243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save_model(model, \"models/mf_01.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd189a9-8169-4bdf-b78a-9644f762fb17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = load_model(\"models/mf_01.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede86748-35de-4d84-a8ba-f85d868184ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reco_env import RecoEnv\n",
    "from utils import import_data_for_env\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05327d09-72eb-49c7-b89a-855d078c9bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(RecoEnv.id, **import_data_for_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e172d7-f8dc-47c9-afd4-7e41ffac982f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d11e8-775d-4c6b-aa99-898b13d1c969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vc = rec.user_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ceea8a-8ecd-45db-b28f-162cfcfe12f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005e4e0-c465-40ee-b22e-a4e54b14173b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vc[vc >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d749a-43b7-4454-bd1e-b3e543020504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10cb107-3281-40c0-9037-fed34a026384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
