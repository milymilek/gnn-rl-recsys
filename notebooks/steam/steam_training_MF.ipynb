{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a88688-5214-4ead-a060-4df9633efd89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miha\\Projects\\gnn-rl-recsys\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"../../\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b41f0f27-4278-4c12-845c-2655b9edcad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f8c1064ef0>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "import operator\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.functional import pad\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec6ccb1-bebd-4890-adb6-64417a22ed65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af40eaa-9d12-46b3-8ffd-3115d7134536",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/steam/data.pkl\", \"rb\") as f:\n",
    "    data = pd.read_pickle(f)\n",
    "    #data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0924f7d8-7845-40a4-b0e5-0b347ff45457",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_col = 'user_id'\n",
    "item_col = 'app_id'\n",
    "train_set = data['train_set'][[user_col, item_col]].values.T\n",
    "supervision_set = data['supervision_set'][[user_col, item_col]].values.T\n",
    "valid_set = data['valid_set'][[user_col, item_col]].values.T\n",
    "user_attr = data['user_attr']\n",
    "item_attr = data['item_attr']\n",
    "\n",
    "train_set = np.concatenate([train_set, supervision_set], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec240adc-0d43-4a65-b58c-16f2e5174f33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scaler_user = StandardScaler()\n",
    "# user_attr_preprocess = scaler_user.fit_transform(user_attr)\n",
    "\n",
    "# scaler_item = StandardScaler()\n",
    "# item_attr_preprocess = np.copy(item_attr)\n",
    "# item_attr_preprocess[:, 435:] = scaler_item.fit_transform(item_attr[:, 435:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "263f75ed-1839-436f-8189-ce7330522ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_item_attr_rebuild(item_attr):\n",
    "    new_item_attr = []\n",
    "    for row in item_attr:\n",
    "        tags = row[15]\n",
    "        OS = list(row[0:3].nonzero()[0])\n",
    "        avg_rat = row[4:10].nonzero()[0][0]\n",
    "        price_original = row[12]\n",
    "        new_item_attr.append([tags, OS, avg_rat, price_original])\n",
    "    return np.array(new_item_attr, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1468d58-223c-401b-8744-b57a11308b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_attr2 = temp_item_attr_rebuild(item_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bf87c72a-70b9-4dfd-978f-176443a485ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([382, 345, 380, 228, 169, 12, 21, 322, 167, 157, 295, 74, 120, 356, 110, 222, 321, 72, 158, 370]),\n",
       "       list([0, 1, 2]), 3, 29.99], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_attr2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "686299d7-371c-46a0-bf2f-2282b7cfb8b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheme = {\n",
    "    \"user_id\": {\n",
    "        \"emb_dim\": 8,\n",
    "        \"num_emb\": user_attr.shape[0],\n",
    "        \"max_len\": 1,\n",
    "        \"type\": 'sparse'\n",
    "    },\n",
    "    \"item_id\": {\n",
    "        \"emb_dim\": 8,\n",
    "        \"num_emb\": item_attr.shape[0],\n",
    "        \"max_len\": 1,\n",
    "        \"type\": 'sparse'\n",
    "    },\n",
    "    \"tags\": {\n",
    "        \"emb_dim\": 8,\n",
    "        \"num_emb\": 425,\n",
    "        \"max_len\": 20,\n",
    "        \"type\": 'sparse'\n",
    "    },\n",
    "    \"OS\": {\n",
    "        \"emb_dim\": 8,\n",
    "        \"num_emb\": 3,\n",
    "        \"max_len\": 3,\n",
    "        \"type\": 'sparse'\n",
    "    },\n",
    "    \"AvgCatRating\": {\n",
    "        \"emb_dim\": 8,\n",
    "        \"num_emb\": 6,\n",
    "        \"max_len\": 1,\n",
    "        \"type\": 'sparse'\n",
    "    },\n",
    "    \"PriceOriginal\": {\n",
    "        \"type\": 'dense'\n",
    "    }\n",
    "}\n",
    "\n",
    "class SparseFeat(namedtuple('SparseFeat', ['name', 'emb_dim', 'num_emb', \"max_len\", \"index\", 'pad_index'])):\n",
    "    def __new__(cls, name, emb_dim, num_emb, max_len, index, pad_index):\n",
    "        return super(SparseFeat, cls).__new__(cls, name, emb_dim, num_emb, max_len, index, pad_index)\n",
    "    \n",
    "class DenseFeat(namedtuple('DenseFeat', ['name', \"index\", 'pad_index'])):\n",
    "    def __new__(cls, name, index, pad_index):\n",
    "        return super(DenseFeat, cls).__new__(cls, name, index, pad_index)    \n",
    "    \n",
    "# class VarLenSparseFeat(namedtuple('VarLenSparseFeat', ['name', 'emb_dim', 'num_emb', \"max_len\", 'index'])):\n",
    "#     def __new__(cls, name, emb_dim, num_emb, max_len, index):\n",
    "#         return super(VarLenSparseFeat, cls).__new__(cls, name, emb_dim, num_emb, max_len, index)\n",
    "\n",
    "# TODO: start+=feat['max_len'] powoduje blad, do sprawdzenia\n",
    "class FeatureStore:\n",
    "    def __init__(self, scheme):\n",
    "        self.features = {'sparse': [], 'dense': []}\n",
    "        self.n_features = len(scheme.keys())\n",
    "        \n",
    "        start = 0\n",
    "        for i, (feat_name, feat) in enumerate(scheme.items()):\n",
    "            if feat['type'] == 'sparse':\n",
    "                self.features['sparse'].append(\n",
    "                    SparseFeat(feat_name, feat['emb_dim'], feat['num_emb'], feat['max_len'], i, (start, start+feat['max_len']))\n",
    "                )\n",
    "                start = start + feat['max_len']\n",
    "            elif feat['type'] == 'dense':\n",
    "                self.features['dense'].append(\n",
    "                    DenseFeat(feat_name, i, (start, start+1))\n",
    "                )\n",
    "                start += 1\n",
    "\n",
    "        self.sparse_index = self.get_feature_index('sparse')\n",
    "        self.dense_index = self.get_feature_index('dense')\n",
    "\n",
    "    def input_len(self):\n",
    "        sparse_len = sum([f.emb_dim for f in self.features['sparse']])\n",
    "        dense_len = len(self.features['dense'])\n",
    "        return sparse_len + dense_len\n",
    "    \n",
    "    def emb_dim(self):\n",
    "        return 8\n",
    "    \n",
    "    def num_emb(self):\n",
    "        return sum([f.num_emb for f in self.features['sparse']])\n",
    "\n",
    "    def get_feature_index(self, feat_type):\n",
    "        return torch.hstack([torch.arange(i[0], i[1]) for i in [f.pad_index for f in self.features[feat_type]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "0870a85b-b7ac-4eb9-b49b-5b6caedf8fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([26])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_store = FeatureStore(scheme)\n",
    "feature_store.dense_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "31739385-daee-4693-ad46-180eb9c775d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepFMDataset(Dataset):\n",
    "    def __init__(self, edge_index, user_attr, item_attr):\n",
    "        self.edge_index = edge_index\n",
    "        self.user_attr = user_attr\n",
    "        self.item_attr = item_attr\n",
    "        \n",
    "        self.users = edge_index[0]\n",
    "        self.items = edge_index[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.edge_index.shape[1]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        u_id = self.users[idx]\n",
    "        i_id = self.items[idx]\n",
    "        \n",
    "        #u_attr = self.user_attr[u_id]\n",
    "        u_attr = []\n",
    "        i_attr = self.item_attr[i_id]\n",
    "\n",
    "        x = np.concatenate([[u_id], [i_id], u_attr, i_attr])\n",
    "        \n",
    "        sparse_features = self._get_sparse_features(x)\n",
    "        dense_features = self._get_dense_features(x)\n",
    "        \n",
    "        return sparse_features, dense_features\n",
    "    \n",
    "    def _get_sparse_features(self, x):\n",
    "        indices = [f.index for f in feature_store.features['sparse']]\n",
    "        return x[indices]\n",
    "    \n",
    "    def _get_dense_features(self, x):\n",
    "        indices = [f.index for f in feature_store.features['dense']]\n",
    "        return x[indices]\n",
    "\n",
    "def pad_sequence_max_len(sequence, max_len):\n",
    "    sequence[0] = pad(sequence[0], (0, max_len-sequence[0].shape[0]), value=-1)\n",
    "    return pad_sequence(sequence, batch_first=True, padding_value=-1)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xs = [[] for f in range(feature_store.n_features)]\n",
    "    for (_sparse, _dense) in batch:\n",
    "        for i, f in enumerate(feature_store.features['sparse']):\n",
    "            xs[f.index].append(torch.tensor(_sparse[i], dtype=torch.float32))\n",
    "\n",
    "        for i, f in enumerate(feature_store.features['dense']):\n",
    "            xs[f.index].append(torch.tensor(_dense[i], dtype=torch.float32))\n",
    "\n",
    "    for i, f in enumerate(feature_store.features['sparse']):\n",
    "        if f.max_len > 1:\n",
    "            xs[f.index] = pad_sequence_max_len(xs[f.index], f.max_len)\n",
    "        else:\n",
    "            xs[f.index] = torch.tensor(xs[f.index])\n",
    "        \n",
    "    for i, f in enumerate(feature_store.features['dense']):\n",
    "        xs[f.index] = torch.tensor(xs[f.index])\n",
    "    \n",
    "    xs = torch.column_stack(xs)\n",
    "\n",
    "    return xs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "a52daab4-3423-40f8-923a-2c1bf17c349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = [torch.tensor([1]), torch.tensor([4,5])]\n",
    "# def pad_sequence_max_len(sequence, max_len):\n",
    "#     sequence[0] = pad(sequence[0], (0, max_len-sequence[0].shape[0]), value=-1)\n",
    "#     return pad_sequence(sequence, batch_first=True, padding_value=-1)\n",
    "# pad_sequence_max_len(l, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "dca648d8-4f41-4dfb-a0ce-1f8c062430b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = DeepFMDataset(train_set, user_attr, item_attr2)\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=1024, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "e5119b48-cc47-457a-95de-9e535e8c2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89788e-35b0-4b42-b95a-e95d7156c7c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_size):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.u = nn.Embedding(n_users, emb_size)\n",
    "        self.i = nn.Embedding(n_items, emb_size)\n",
    "        \n",
    "    def forward(self, ux, ix):\n",
    "        return torch.sum(self.u(ux) * self.i(ix), dim=1)\n",
    "        #return torch.sum((self.u(ux) * self.i(ix)).squeeze(1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6897b831-30f5-471a-b558-056d6a65ae1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3141a2b0-dc80-45c9-9f64-b0dc9b2882f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingBagCollection:\n",
    "    \"\"\"\n",
    "    EmbeddingBagCollection represents a collection of pooled embeddings (`EmbeddingBags`).\n",
    "\n",
    "    It processes sparse data in the form of `KeyedJaggedTensor` with values of the form\n",
    "    [F X B X L] where:\n",
    "\n",
    "    * F: features (keys)\n",
    "    * B: batch size\n",
    "    * L: length of sparse features (jagged)\n",
    "\n",
    "    and outputs a `KeyedTensor` with values of the form [B * (F * D)] where:\n",
    "\n",
    "    * F: features (keys)\n",
    "    * D: each feature's (key's) embedding dimension\n",
    "    * B: batch size\n",
    "\n",
    "    Args:\n",
    "        tables (List[EmbeddingBagConfig]): list of embedding tables.\n",
    "        is_weighted (bool): whether input `KeyedJaggedTensor` is weighted.\n",
    "        device (Optional[torch.device]): default compute device.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        table_0 = EmbeddingBagConfig(\n",
    "            name=\"t1\", embedding_dim=3, num_embeddings=10, feature_names=[\"f1\"]\n",
    "        )\n",
    "        table_1 = EmbeddingBagConfig(\n",
    "            name=\"t2\", embedding_dim=4, num_embeddings=10, feature_names=[\"f2\"]\n",
    "        )\n",
    "\n",
    "        ebc = EmbeddingBagCollection(tables=[table_0, table_1])\n",
    "\n",
    "        #        0       1        2  <-- batch\n",
    "        # \"f1\"   [0,1] None    [2]\n",
    "        # \"f2\"   [3]    [4]    [5,6,7]\n",
    "        #  ^\n",
    "        # feature\n",
    "\n",
    "        features = KeyedJaggedTensor(\n",
    "            keys=[\"f1\", \"f2\"],\n",
    "            values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7]),\n",
    "            offsets=torch.tensor([0, 2, 2, 3, 4, 5, 8]),\n",
    "        )\n",
    "\n",
    "        pooled_embeddings = ebc(features)\n",
    "        print(pooled_embeddings.values())\n",
    "        tensor([[-0.8899, -0.1342, -1.9060, -0.0905, -0.2814, -0.9369, -0.7783],\n",
    "            [ 0.0000,  0.0000,  0.0000,  0.1598,  0.0695,  1.3265, -0.1011],\n",
    "            [-0.4256, -1.1846, -2.1648, -1.0893,  0.3590, -1.9784, -0.7681]],\n",
    "            grad_fn=<CatBackward0>)\n",
    "        print(pooled_embeddings.keys())\n",
    "        ['f1', 'f2']\n",
    "        print(pooled_embeddings.offset_per_key())\n",
    "        tensor([0, 3, 7])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_store: FeatureStore) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_bags: nn.ModuleDict = nn.ModuleDict()\n",
    "        self._feature_store = feature_store\n",
    "        self._lengths_per_embedding: List[int] = []\n",
    "\n",
    "        table_names = set()\n",
    "        for embedding_config in tables:\n",
    "            if embedding_config.name in table_names:\n",
    "                raise ValueError(f\"Duplicate table name {embedding_config.name}\")\n",
    "            table_names.add(embedding_config.name)\n",
    "            dtype = (\n",
    "                torch.float32\n",
    "                if embedding_config.data_type == DataType.FP32\n",
    "                else torch.float16\n",
    "            )\n",
    "            self.embedding_bags[embedding_config.name] = nn.EmbeddingBag(\n",
    "                num_embeddings=embedding_config.num_embeddings,\n",
    "                embedding_dim=embedding_config.embedding_dim,\n",
    "                mode=pooling_type_to_str(embedding_config.pooling),\n",
    "                device=self._device,\n",
    "                include_last_offset=True,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "\n",
    "            if not embedding_config.feature_names:\n",
    "                embedding_config.feature_names = [embedding_config.name]\n",
    "            self._lengths_per_embedding.extend(\n",
    "                len(embedding_config.feature_names) * [embedding_config.embedding_dim]\n",
    "            )\n",
    "\n",
    "        self._embedding_names: List[str] = [\n",
    "            embedding\n",
    "            for embeddings in get_embedding_names_by_table(tables)\n",
    "            for embedding in embeddings\n",
    "        ]\n",
    "        self._feature_names: List[List[str]] = [table.feature_names for table in tables]\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, features: KeyedJaggedTensor) -> KeyedTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (KeyedJaggedTensor): KJT of form [F X B X L].\n",
    "\n",
    "        Returns:\n",
    "            KeyedTensor\n",
    "        \"\"\"\n",
    "\n",
    "        pooled_embeddings: List[torch.Tensor] = []\n",
    "\n",
    "        feature_dict = features.to_dict()\n",
    "        for i, embedding_bag in enumerate(self.embedding_bags.values()):\n",
    "            for feature_name in self._feature_names[i]:\n",
    "                f = feature_dict[feature_name]\n",
    "                res = embedding_bag(\n",
    "                    input=f.values(),\n",
    "                    offsets=f.offsets(),\n",
    "                    per_sample_weights=f.weights() if self._is_weighted else None,\n",
    "                ).float()\n",
    "                pooled_embeddings.append(res)\n",
    "        data = torch.cat(pooled_embeddings, dim=1)\n",
    "        return KeyedTensor(\n",
    "            keys=self._embedding_names,\n",
    "            values=data,\n",
    "            length_per_key=self._lengths_per_embedding,\n",
    "        )\n",
    "\n",
    "    def is_weighted(self) -> bool:\n",
    "        return self._is_weighted\n",
    "\n",
    "    def embedding_bag_configs(self) -> List[EmbeddingBagConfig]:\n",
    "        return self._embedding_bag_configs\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self._device\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        if (isinstance(self.device, torch.device) and self.device.type == \"meta\") or (\n",
    "            isinstance(self.device, str) and self.device == \"meta\"\n",
    "        ):\n",
    "            return\n",
    "        # Initialize embedding bags weights with init_fn\n",
    "        for table_config in self._embedding_bag_configs:\n",
    "            assert table_config.init_fn is not None\n",
    "            param = self.embedding_bags[f\"{table_config.name}\"].weight\n",
    "            # pyre-ignore\n",
    "            table_config.init_fn(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f9902-3592-4ce0-b29e-12d4c6102ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CrossProductNet(nn.Module):\n",
    "    def forward(self, emb_x):\n",
    "        cross = emb_x @ emb_x.permute(1, 0)\n",
    "        triangle_lower_cross = torch.tril(cross, diagonal=-1)\n",
    "        return torch.sum(triangle_lower_cross)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.1, batch_norm=True):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        _layers = []\n",
    "        dims = [input_dim] + hidden_dim + [1]\n",
    "        for in_dim, out_dim in zip(dims, dims[1:]):\n",
    "            _layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if batch_norm:\n",
    "                _layers.append(nn.BatchNorm1d(out_dim))\n",
    "            _layers.append(nn.ReLU())\n",
    "            _layers.append(nn.Dropout(p=dropout))\n",
    "\n",
    "        self.layers = nn.ModuleList(_layers)\n",
    "        \n",
    "    def forward(self, emb_x):\n",
    "        return self.layers(emb_x)\n",
    "    \n",
    "# class EmbeddingNet(nn.Module):\n",
    "#     def __init__(self, features):\n",
    "#         super(EmbeddingNet, self).__init__()\n",
    "#         _embeddings = {feat.name: nn.Embedding(feat.emb_values, feat.emb_dim) for feat in features}\n",
    "#         self.features = features\n",
    "#         self.embeddings = nn.ModuleDict(_embeddings)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         emb = []\n",
    "#         for name, embed_matrix in self.embeddings.items():\n",
    "#             if isinstance(x[name], list):\n",
    "#                 emb.append(torch.sum(embed_matrix(torch.tensor(x[name])), axis=0))\n",
    "#             else:\n",
    "#                 emb.append(embed_matrix(torch.tensor(x[name])))\n",
    "#         return emb\n",
    "\n",
    "# class EmbeddingNet(nn.Module):\n",
    "#     def __init__(self, sparse_feat, varlen_feat, emb_dim):\n",
    "#         super(EmbeddingNet, self).__init__()\n",
    "#         self.sparse_emb = nn.Embedding(sparse_feat, emb_dim)\n",
    "#         self.varlen_sparse_emb = nn.Embedding(varlen_feat, emb_dim)\n",
    "        \n",
    "#         self.offsets_sparse = np.array((0, *np.cumsum(sparse_feat)[:-1]))\n",
    "#         self.offsets_varlen = np.array((0, *np.cumsum(varlen_feat)[:-1]))\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "#         return self.embedding(x)       \n",
    "\n",
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, feature_store, hidden_dim):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.feature_store = feature_store\n",
    "\n",
    "        self.V = EmbeddingNet(sparse_num_emb, varlen_num_emb, emb_dim)\n",
    "        self.fm = CrossProductNet()\n",
    "        self.dnn = MLP(input_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_sparse, x_dense = x[feature_store.sparse_index], x[feature_store.dense_index]\n",
    "        \n",
    "        x_sparse = self.V(x_sparse)\n",
    "        x_sparse_dense = torch.cat([x_sparse, x_dense])\n",
    "        x = self.fm(x_sparse_emb) + self.dnn(x_sparse_dense)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b60322-6a67-44f3-a842-4108362cdc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepFM(feature_store, hidden_dim=[128,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd6081-ce07-4dae-994b-c089d2c5eaa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.tensor([[1,2,3], [4,5,6], [7,8,9]])[[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b9834-c97c-4a32-85f7-e03fd51deddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57202cec-db0d-47a3-a23b-4cc8851573b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65451b50-2b21-4b72-8954-95c53faf8f27",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset[9432]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0114530-1d6a-4353-8167-2683b2a31d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_input_types(train_dataset[9432], features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a6d8a-4aec-4366-a39b-7f8fad51e9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704dadb-a8d7-4257-abaa-4eeb8479c77e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "e = EmbeddingNet([f.num_emb for f in features], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d2037-ff6b-4cea-bfc8-b3524fd6ae50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.array((0, *np.cumsum([f.num_emb for f in features])[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937ac4f-e444-44c8-b70b-8c52cfc65fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[f.num_emb for f in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8c38a-3050-438d-a96d-f4666c214028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "e.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2499c-ce55-4ec8-9144-37e9dad129ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.arange(20).new_tensor([0, 5, 10]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3948e919-3d98-4e28-9f78-8ed8cad9b90d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_input = {\n",
    "    \"user_id\": 5,\n",
    "    \"item_id\": 20,\n",
    "    \"tags\": [0, 5, 8, 100],\n",
    "    \"OS\": [0, 1],\n",
    "    \"AvgCatRating\": 3 \n",
    "}\n",
    "x_input_tensor = torch.tensor([5, \n",
    "                               20, \n",
    "                               0, 5, 8, 100,\n",
    "                               0, 1,\n",
    "                               3\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b952a6b-4128-4cdf-a76c-b59b27a5421f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "e(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2608366-6dfa-45bb-bf72-e1c4a4e55138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def approx_negative_sampling(u_id, i_id, n_samples, bs):\n",
    "    ind = torch.arange(bs).repeat(n_samples)\n",
    "    perm =  torch.randperm(ind.shape[0])\n",
    "    random_indices = ind[perm]\n",
    "    i_id_neg = i_id[random_indices]\n",
    "    \n",
    "    u_id = u_id.repeat(n_samples+1)\n",
    "    i_id = torch.cat([i_id, i_id_neg])\n",
    "    y_true = torch.cat([torch.ones(bs).to(device),\n",
    "                       torch.zeros(bs*n_samples).to(device)])\n",
    "    \n",
    "    return u_id, i_id, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d8d8c-9497-4114-be44-dd32bf945e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(n_epochs, print_loss=500):\n",
    "    model.train()\n",
    "    batch_size = train_loader.batch_size\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.\n",
    "        preds, ground_truths = [], []\n",
    "        batch_print_loss = 0.\n",
    "        for i_batch, batch in enumerate(tqdm(train_loader)):\n",
    "            batch = [i.to(device) for i in batch]\n",
    "            u_id, i_id, u_attr, i_attr = batch\n",
    " \n",
    "            u_id, i_id, y_true = approx_negative_sampling(u_id, i_id, n_samples=2, bs=batch_size)\n",
    "            \n",
    "            y_pred = model(u_id, i_id)\n",
    "            loss = criterion(y_pred, y_true)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            preds.append(y_pred)\n",
    "            ground_truths.append(y_true)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if not ((i_batch+1) % print_loss):\n",
    "                pred = torch.cat(preds, dim=0).detach().sigmoid().cpu().numpy()\n",
    "                ground_truth = torch.cat(ground_truths, dim=0).detach().cpu().numpy()\n",
    "                last_loss = running_loss / print_loss\n",
    "                \n",
    "                train_roc_auc = roc_auc_score(ground_truth, pred)\n",
    "                test_loss, test_roc_auc = test()\n",
    "                \n",
    "                preds, ground_truths = [], []\n",
    "                running_loss = 0.\n",
    "                \n",
    "                print(f\"batch <{i_batch}>\\ntrain_loss: {last_loss} - train_roc_auc: {train_roc_auc}\\ntest_loss: {test_loss} - test_roc_auc: {test_roc_auc}\\n\")\n",
    "        print(f\"Epoch: {epoch}, Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79f0cd-9f28-498a-b764-cf0fb3117f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    batch_size = val_loader.batch_size\n",
    "    \n",
    "    running_loss = 0.\n",
    "    preds, ground_truths = [], []\n",
    "\n",
    "    for i_batch, batch in enumerate(tqdm(val_loader)):\n",
    "        batch = [i.to(device) for i in batch]\n",
    "        u_id, i_id, u_attr, i_attr = batch\n",
    "        \n",
    "        u_id, i_id, y_true = approx_negative_sampling(u_id, i_id, n_samples=2, bs=batch_size)\n",
    "\n",
    "        y_pred = model(u_id, i_id)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        \n",
    "        preds.append(y_pred)\n",
    "        ground_truths.append(y_true)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    \n",
    "    test_loss = running_loss / len(val_loader)\n",
    "    test_score = roc_auc_score(ground_truth, pred)\n",
    "\n",
    "    return test_loss, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29807538-6259-4fed-819b-acab8341ba21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = MFDataset(train_set, data['user_attr'], data['item_attr'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, drop_last=True)\n",
    "\n",
    "val_dataset = MFDataset(valid_set, data['user_attr'], data['item_attr'])\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e7247-d351-4d12-ad27-2b072f322bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_users, n_items = user_attr.shape[0], item_attr.shape[0]\n",
    "\n",
    "model = MF(n_users, n_items, emb_size=32)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42106db4-3626-44c7-b088-4fa4fd1144c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.RMSprop(params=model.parameters(), lr=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3c283-8dc0-4ea2-bc2f-6cc6ee1e9116",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(n_epochs=10, print_loss=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f3a58e-b222-4c5c-8aa4-6f3cd8be8b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabdfbbb-1002-4098-81a9-b00b4da58d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfecea9-ed11-42d9-87e3-c338303fa39d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch_size = train_loader.batch_size\n",
    "    batch = next(iter(train_loader))\n",
    "    batch = [i.to(device) for i in batch]\n",
    "    u_id, i_id, u_attr, i_attr = batch\n",
    "    \n",
    "    neg_item_idx = approx_negative_sampling(i_id, bs=batch_size)\n",
    "\n",
    "    u_id = torch.cat([u_id, u_id])\n",
    "    i_id = torch.cat([i_id, neg_item_idx])\n",
    "    y_true = torch.cat([torch.ones(batch_size).to(device),\n",
    "                        torch.zeros(batch_size).to(device)])\n",
    "\n",
    "    y_pred = model(u_id, i_id)\n",
    "    \n",
    "    print(y_pred.sigmoid(), y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca6fe9-55a6-4b7b-9ef6-48c1ef92aa5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7372b8f-3328-4d4f-a27d-bee6c00f76cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd7dd4-b196-4540-8eef-843f90ef523c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "def load_model(path):\n",
    "    model = MF(n_users, n_items, emb_size=32)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3819672-0ed1-4ab9-bd16-076184652243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save_model(model, \"models/mf_01.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd189a9-8169-4bdf-b78a-9644f762fb17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = load_model(\"models/mf_01.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede86748-35de-4d84-a8ba-f85d868184ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reco_env import RecoEnv\n",
    "from utils import import_data_for_env\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05327d09-72eb-49c7-b89a-855d078c9bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(RecoEnv.id, **import_data_for_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e172d7-f8dc-47c9-afd4-7e41ffac982f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d11e8-775d-4c6b-aa99-898b13d1c969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vc = rec.user_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ceea8a-8ecd-45db-b28f-162cfcfe12f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005e4e0-c465-40ee-b22e-a4e54b14173b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vc[vc >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d749a-43b7-4454-bd1e-b3e543020504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10cb107-3281-40c0-9037-fed34a026384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
