{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a88688-5214-4ead-a060-4df9633efd89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miha\\Projects\\gnn-rl-recsys\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"../../\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b41f0f27-4278-4c12-845c-2655b9edcad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x213e55f4ed0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "import operator\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.functional import pad\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec6ccb1-bebd-4890-adb6-64417a22ed65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2af40eaa-9d12-46b3-8ffd-3115d7134536",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/steam/data.pkl\", \"rb\") as f:\n",
    "    data = pd.read_pickle(f)\n",
    "    #data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0924f7d8-7845-40a4-b0e5-0b347ff45457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_col = 'user_id'\n",
    "item_col = 'app_id'\n",
    "train_set = data['train_set'][[user_col, item_col]].values.T\n",
    "supervision_set = data['supervision_set'][[user_col, item_col]].values.T\n",
    "valid_set = data['valid_set'][[user_col, item_col]].values.T\n",
    "user_attr = data['user_attr']\n",
    "item_attr = data['item_attr']\n",
    "\n",
    "train_set = np.concatenate([train_set, supervision_set], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec240adc-0d43-4a65-b58c-16f2e5174f33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scaler_user = StandardScaler()\n",
    "# user_attr_preprocess = scaler_user.fit_transform(user_attr)\n",
    "\n",
    "# scaler_item = StandardScaler()\n",
    "# item_attr_preprocess = np.copy(item_attr)\n",
    "# item_attr_preprocess[:, 435:] = scaler_item.fit_transform(item_attr[:, 435:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "263f75ed-1839-436f-8189-ce7330522ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def temp_item_attr_rebuild(item_attr):\n",
    "    new_item_attr = []\n",
    "    for row in item_attr:\n",
    "        tags = row[15]\n",
    "        OS = list(row[0:3].nonzero()[0])\n",
    "        avg_rat = row[4:10].nonzero()[0][0]\n",
    "        price_original = row[12]\n",
    "        new_item_attr.append([tags, OS, avg_rat, price_original])\n",
    "    return np.array(new_item_attr, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1468d58-223c-401b-8744-b57a11308b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_attr2 = temp_item_attr_rebuild(item_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686299d7-371c-46a0-bf2f-2282b7cfb8b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheme = {\n",
    "    \"user_id\": {\n",
    "        \"emb_dim\": 8,\n",
    "        \"num_emb\": user_attr.shape[0],\n",
    "        \"max_len\": 1,\n",
    "        \"type\": 'sparse'\n",
    "    },\n",
    "    \"item_id\": {\n",
    "        \"emb_dim\": 8,\n",
    "        \"num_emb\": item_attr.shape[0],\n",
    "        \"max_len\": 1,\n",
    "        \"type\": 'sparse'\n",
    "    },\n",
    "    # \"tags\": {\n",
    "    #     \"emb_dim\": 4,\n",
    "    #     \"num_emb\": 425,\n",
    "    #     \"max_len\": 20,\n",
    "    #     \"type\": 'sparse'\n",
    "    # },\n",
    "    # \"OS\": {\n",
    "    #     \"emb_dim\": 4,\n",
    "    #     \"num_emb\": 3,\n",
    "    #     \"max_len\": 3,\n",
    "    #     \"type\": 'sparse'\n",
    "    # },\n",
    "    # \"AvgCatRating\": {\n",
    "    #     \"emb_dim\": 4,\n",
    "    #     \"num_emb\": 6,\n",
    "    #     \"max_len\": 1,\n",
    "    #     \"type\": 'sparse'\n",
    "    # },\n",
    "    # \"PriceOriginal\": {\n",
    "    #     \"type\": 'dense'\n",
    "    # }\n",
    "}\n",
    "\n",
    "class SparseFeat(namedtuple('SparseFeat', ['name', 'emb_dim', 'num_emb', \"max_len\", \"index\", 'pad_index'])):\n",
    "    def __new__(cls, name, emb_dim, num_emb, max_len, index, pad_index):\n",
    "        return super(SparseFeat, cls).__new__(cls, name, emb_dim, num_emb, max_len, index, pad_index)\n",
    "    \n",
    "class DenseFeat(namedtuple('DenseFeat', ['name', \"index\", 'pad_index'])):\n",
    "    def __new__(cls, name, index, pad_index):\n",
    "        return super(DenseFeat, cls).__new__(cls, name, index, pad_index)    \n",
    "\n",
    "# TODO: start+=feat['max_len'] powoduje blad, do sprawdzenia\n",
    "class FeatureStore:\n",
    "    def __init__(self, scheme):\n",
    "        self.features = {'sparse': [], 'dense': []}\n",
    "        self.n_features = len(scheme.keys())\n",
    "        \n",
    "        start = 0\n",
    "        for i, (feat_name, feat) in enumerate(scheme.items()):\n",
    "            if feat['type'] == 'sparse':\n",
    "                self.features['sparse'].append(\n",
    "                    SparseFeat(feat_name, feat['emb_dim'], feat['num_emb'], feat['max_len'], i, (start, start+feat['max_len']))\n",
    "                )\n",
    "                start = start + feat['max_len']\n",
    "            elif feat['type'] == 'dense':\n",
    "                self.features['dense'].append(\n",
    "                    DenseFeat(feat_name, i, (start, start+1))\n",
    "                )\n",
    "                start += 1\n",
    "\n",
    "        self.sparse_index = self.get_feature_index('sparse')\n",
    "        self.dense_index = self.get_feature_index('dense')\n",
    "\n",
    "    def input_len(self):\n",
    "        sparse_len = sum([f.emb_dim for f in self.features['sparse']])\n",
    "        dense_len = len(self.features['dense'])\n",
    "        return sparse_len + dense_len\n",
    "    \n",
    "    def emb_dim(self):\n",
    "        return 8\n",
    "    \n",
    "    def num_emb(self):\n",
    "        return sum([f.num_emb for f in self.features['sparse']])\n",
    "\n",
    "    def get_feature_index(self, feat_type):\n",
    "        return torch.hstack([torch.arange(i[0], i[1]) for i in [f.pad_index for f in self.features[feat_type]]])\n",
    "\n",
    "    def get_input_dim(self):\n",
    "        return sum([f.emb_dim for f in self.features['sparse']]) + len(self.features['dense'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0870a85b-b7ac-4eb9-b49b-5b6caedf8fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "hstack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m feature_store \u001b[38;5;241m=\u001b[39m \u001b[43mFeatureStore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheme\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 65\u001b[0m, in \u001b[0;36mFeatureStore.__init__\u001b[1;34m(self, scheme)\u001b[0m\n\u001b[0;32m     62\u001b[0m         start \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_feature_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdense\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 79\u001b[0m, in \u001b[0;36mFeatureStore.get_feature_index\u001b[1;34m(self, feat_type)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_feature_index\u001b[39m(\u001b[38;5;28mself\u001b[39m, feat_type):\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeat_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: hstack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "feature_store = FeatureStore(scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31739385-daee-4693-ad46-180eb9c775d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepFMDataset(Dataset):\n",
    "    def __init__(self, edge_index, user_attr, item_attr):\n",
    "        self.edge_index = edge_index\n",
    "        self.user_attr = user_attr\n",
    "        self.item_attr = item_attr\n",
    "        \n",
    "        self.users = edge_index[0]\n",
    "        self.items = edge_index[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.edge_index.shape[1]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        u_id = self.users[idx]\n",
    "        i_id = self.items[idx]\n",
    "        \n",
    "        #u_attr = self.user_attr[u_id]\n",
    "        u_attr = []\n",
    "        i_attr = self.item_attr[i_id]\n",
    "\n",
    "        x = np.concatenate([[u_id], [i_id], u_attr, i_attr])\n",
    "        \n",
    "        sparse_features = self._get_sparse_features(x)\n",
    "        dense_features = self._get_dense_features(x)\n",
    "        \n",
    "        return sparse_features, dense_features\n",
    "    \n",
    "    def _get_sparse_features(self, x):\n",
    "        indices = [f.index for f in feature_store.features['sparse']]\n",
    "        return x[indices]\n",
    "    \n",
    "    def _get_dense_features(self, x):\n",
    "        indices = [f.index for f in feature_store.features['dense']]\n",
    "        return x[indices]\n",
    "\n",
    "def pad_sequence_max_len(sequence, max_len):\n",
    "    sequence[0] = pad(sequence[0], (0, max_len-sequence[0].shape[0]), value=-1)\n",
    "    return pad_sequence(sequence, batch_first=True, padding_value=-1) + 1\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xs = [[] for f in range(feature_store.n_features)]\n",
    "    for (_sparse, _dense) in batch:\n",
    "        for i, f in enumerate(feature_store.features['sparse']):\n",
    "            xs[f.index].append(torch.tensor(_sparse[i], dtype=torch.float32))\n",
    "\n",
    "        for i, f in enumerate(feature_store.features['dense']):\n",
    "            xs[f.index].append(torch.tensor(_dense[i], dtype=torch.float32))\n",
    "\n",
    "    for i, f in enumerate(feature_store.features['sparse']):\n",
    "        if f.max_len > 1:\n",
    "            xs[f.index] = pad_sequence_max_len(xs[f.index], f.max_len)\n",
    "        else:\n",
    "            xs[f.index] = torch.tensor(xs[f.index]) + 1\n",
    "        \n",
    "    for i, f in enumerate(feature_store.features['dense']):\n",
    "        xs[f.index] = torch.tensor(xs[f.index])\n",
    "    \n",
    "    xs = torch.column_stack(xs)\n",
    "\n",
    "    return xs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89788e-35b0-4b42-b95a-e95d7156c7c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_size):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.u = nn.Embedding(n_users, emb_size)\n",
    "        self.i = nn.Embedding(n_items, emb_size)\n",
    "        \n",
    "    def forward(self, ux, ix):\n",
    "        return torch.sum(self.u(ux) * self.i(ix), dim=1)\n",
    "        #return torch.sum((self.u(ux) * self.i(ix)).squeeze(1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f9902-3592-4ce0-b29e-12d4c6102ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CrossProductNet(nn.Module):\n",
    "    def forward(self, emb_x):\n",
    "        cross = torch.bmm(emb_x.unsqueeze(2), emb_x.unsqueeze(1))\n",
    "        return torch.tensor([torch.tril(i, diagonal=-1).sum() for i in cross], device=device).unsqueeze(1)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.1, batch_norm=True):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        _layers = []\n",
    "        dims = [input_dim] + hidden_dim + [1]\n",
    "        for in_dim, out_dim in zip(dims, dims[1:]):\n",
    "            _layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if batch_norm:\n",
    "                _layers.append(nn.BatchNorm1d(out_dim))\n",
    "            _layers.append(nn.ReLU())\n",
    "            _layers.append(nn.Dropout(p=dropout))\n",
    "\n",
    "        self.layers = nn.Sequential(*_layers)\n",
    "        \n",
    "    def forward(self, emb_x):\n",
    "        return self.layers(emb_x)\n",
    "    \n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, feature_store):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        _embeddings = {feat.name: nn.Embedding(feat.num_emb+1, feat.emb_dim, padding_idx=0, device=device) for feat in feature_store.features['sparse']}\n",
    "        self._feature_store = feature_store\n",
    "        self.embeddings = nn.ModuleDict(_embeddings)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_emb = []\n",
    "        for name, embed_matrix in self.embeddings.items():\n",
    "            feat = [i for i in self._feature_store.features['sparse'] if i.name == name][0]\n",
    "            x_feat = x[:, feat.pad_index[0]:feat.pad_index[1]].to(torch.long)\n",
    "            emb = embed_matrix(x_feat)\n",
    "            emb_agg = torch.mean(emb, axis=1)\n",
    "            x_emb.append(emb_agg)\n",
    "        x_emb = torch.cat(x_emb, axis=1)\n",
    "        return x_emb   \n",
    "\n",
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, feature_store, hidden_dim):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.feature_store = feature_store\n",
    "\n",
    "        self.V = EmbeddingNet(feature_store)\n",
    "        self.fm = CrossProductNet()\n",
    "        self.dnn = MLP(feature_store.get_input_dim(), hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_dense = x[:, feature_store.dense_index]\n",
    "        \n",
    "        x_sparse = self.V(x)\n",
    "        x_sparse_dense = torch.cat([x_sparse, x_dense], dim=1)\n",
    "        \n",
    "        x = self.fm(x_sparse) + self.dnn(x_sparse_dense)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2608366-6dfa-45bb-bf72-e1c4a4e55138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def approx_negative_sampling(batch, n_samples, bs):\n",
    "    ind = torch.arange(bs).repeat(n_samples)\n",
    "    perm =  torch.randperm(ind.shape[0])\n",
    "    random_indices = ind[perm]\n",
    "    i_id_neg = batch[random_indices, 1:] # apply indexing batch[random_indices, 0] to get only item_id \n",
    "                                         # column, else get all columns (currently all item features)\n",
    "    \n",
    "    u_id = batch[:, 0].repeat(n_samples+1).unsqueeze(1)\n",
    "    i_id = torch.cat([batch[:, 1:], i_id_neg])\n",
    "    batch = torch.cat([u_id, i_id], dim=1)\n",
    "    \n",
    "    y_true = torch.cat([torch.ones(bs).to(device),\n",
    "                       torch.zeros(bs*n_samples).to(device)]).unsqueeze(1)\n",
    "    \n",
    "    return batch, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d8d8d8c-9497-4114-be44-dd32bf945e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(n_epochs, print_loss=500):\n",
    "    model.train()\n",
    "    batch_size = train_loader.batch_size\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.\n",
    "        preds, ground_truths = [], []\n",
    "        batch_print_loss = 0.\n",
    "        for i_batch, batch in enumerate(tqdm(train_loader)):\n",
    "            batch, y_true = approx_negative_sampling(batch, n_samples=2, bs=batch_size)\n",
    "            \n",
    "            y_pred = model(batch)\n",
    "            loss = criterion(y_pred, y_true)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            preds.append(y_pred)\n",
    "            ground_truths.append(y_true)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if not ((i_batch+1) % print_loss):\n",
    "                pred = torch.cat(preds, dim=0).detach().sigmoid().cpu().numpy()\n",
    "                ground_truth = torch.cat(ground_truths, dim=0).detach().cpu().numpy()\n",
    "                last_loss = running_loss / print_loss\n",
    "                \n",
    "                train_roc_auc = roc_auc_score(ground_truth, pred)\n",
    "                #test_loss, test_roc_auc = test()\n",
    "                test_loss, test_roc_auc = -1, -1\n",
    "                \n",
    "                preds, ground_truths = [], []\n",
    "                running_loss = 0.\n",
    "                \n",
    "                print(f\"batch <{i_batch}>\\ntrain_loss: {last_loss} - train_roc_auc: {train_roc_auc}\\ntest_loss: {test_loss} - test_roc_auc: {test_roc_auc}\\n\")\n",
    "        print(f\"Epoch: {epoch}, Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d79f0cd-9f28-498a-b764-cf0fb3117f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    batch_size = val_loader.batch_size\n",
    "    \n",
    "    running_loss = 0.\n",
    "    preds, ground_truths = [], []\n",
    "\n",
    "    for i_batch, batch in enumerate(tqdm(val_loader)):\n",
    "        batch = [i.to(device) for i in batch]\n",
    "        u_id, i_id, u_attr, i_attr = batch\n",
    "        \n",
    "        u_id, i_id, y_true = approx_negative_sampling(u_id, i_id, n_samples=2, bs=batch_size)\n",
    "\n",
    "        y_pred = model(u_id, i_id)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        \n",
    "        preds.append(y_pred)\n",
    "        ground_truths.append(y_true)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "    \n",
    "    test_loss = running_loss / len(val_loader)\n",
    "    test_score = roc_auc_score(ground_truth, pred)\n",
    "\n",
    "    return test_loss, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29807538-6259-4fed-819b-acab8341ba21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = MFDataset(train_set, data['user_attr'], data['item_attr'])\n",
    "# train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, drop_last=True)\n",
    "\n",
    "# val_dataset = MFDataset(valid_set, data['user_attr'], data['item_attr'])\n",
    "# val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False, drop_last=True)\n",
    "\n",
    "# n_users, n_items = user_attr.shape[0], item_attr.shape[0]\n",
    "# model = MF(n_users, n_items, emb_size=32)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2b51a3b-c3d8-46ef-8d91-2656251f5ca1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DeepFMDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDeepFMDataset\u001b[49m(train_set, user_attr, item_attr2)\n\u001b[0;32m      2\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m dataloader\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DeepFMDataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = DeepFMDataset(train_set, user_attr, item_attr2)\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=1024, collate_fn=collate_fn, drop_last=True)\n",
    "train_loader = dataloader\n",
    "\n",
    "model = DeepFM(feature_store, hidden_dim=[128, 64])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42106db4-3626-44c7-b088-4fa4fd1144c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.RMSprop(params=model.parameters(), lr=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3c283-8dc0-4ea2-bc2f-6cc6ee1e9116",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(n_epochs=10, print_loss=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabdfbbb-1002-4098-81a9-b00b4da58d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfecea9-ed11-42d9-87e3-c338303fa39d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch_size = train_loader.batch_size\n",
    "    batch = next(iter(train_loader))\n",
    "    batch = [i.to(device) for i in batch]\n",
    "    u_id, i_id, u_attr, i_attr = batch\n",
    "    \n",
    "    neg_item_idx = approx_negative_sampling(i_id, bs=batch_size)\n",
    "\n",
    "    u_id = torch.cat([u_id, u_id])\n",
    "    i_id = torch.cat([i_id, neg_item_idx])\n",
    "    y_true = torch.cat([torch.ones(batch_size).to(device),\n",
    "                        torch.zeros(batch_size).to(device)])\n",
    "\n",
    "    y_pred = model(u_id, i_id)\n",
    "    \n",
    "    print(y_pred.sigmoid(), y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca6fe9-55a6-4b7b-9ef6-48c1ef92aa5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7372b8f-3328-4d4f-a27d-bee6c00f76cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd7dd4-b196-4540-8eef-843f90ef523c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "def load_model(path):\n",
    "    model = MF(n_users, n_items, emb_size=32)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3819672-0ed1-4ab9-bd16-076184652243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save_model(model, \"models/mf_01.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd189a9-8169-4bdf-b78a-9644f762fb17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = load_model(\"models/mf_01.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede86748-35de-4d84-a8ba-f85d868184ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reco_env import RecoEnv\n",
    "from utils import import_data_for_env\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05327d09-72eb-49c7-b89a-855d078c9bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(RecoEnv.id, **import_data_for_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e172d7-f8dc-47c9-afd4-7e41ffac982f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d11e8-775d-4c6b-aa99-898b13d1c969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vc = rec.user_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ceea8a-8ecd-45db-b28f-162cfcfe12f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005e4e0-c465-40ee-b22e-a4e54b14173b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vc[vc >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d749a-43b7-4454-bd1e-b3e543020504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10cb107-3281-40c0-9037-fed34a026384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
